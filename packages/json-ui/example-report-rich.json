{
  "type": "Report",
  "props": { "theme": "auto" },
  "children": [
    {
      "type": "BrandHeader",
      "props": {
        "badge": { "en": "ğŸ¤– AI Generated Paper Summary", "zh": "ğŸ¤– AI ç”Ÿæˆè®ºæ–‡è§£è¯»" },
        "poweredBy": "ActionBook"
      }
    },
    {
      "type": "PaperHeader",
      "props": {
        "title": { "en": "Your Group-Relative Advantage Is Biased", "zh": "ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å­˜åœ¨åå·®" },
        "arxivId": "2601.08521",
        "date": "2026-01-13",
        "categories": ["cs.LG", "cs.AI", "stat.ML"],
        "version": "v1"
      }
    },
    {
      "type": "AuthorList",
      "props": {
        "authors": [
          { "name": "Fengkai Yang", "affiliation": "Beihang University" },
          { "name": "Zherui Chen", "affiliation": "UC Berkeley" },
          { "name": "Xiaohan Wang", "affiliation": "Peking University" },
          { "name": "Xiaodong Lu", "affiliation": "Meituan" },
          { "name": "Jiajun Chai" },
          { "name": "Guojun Yin" },
          { "name": "Wei Lin" }
        ],
        "maxVisible": 4
      }
    },
    {
      "type": "Section",
      "props": { "title": { "en": "Abstract", "zh": "æ‘˜è¦" }, "icon": "paper" },
      "children": [
        {
          "type": "Abstract",
          "props": {
            "text": {
              "en": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood. In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide theoretical analysis showing this bias systematically disadvantages questions of moderate difficulty while over-rewarding both easy and hard questions. To address this, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), which dynamically adjusts advantage weights using an evolving difficulty anchor. Experiments across five mathematical reasoning benchmarks show consistent improvements over GRPO and other baselines.",
              "zh": "åŸºäºéªŒè¯å™¨å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å·²æˆä¸ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†ä»»åŠ¡åè®­ç»ƒçš„ä¸»æµæ–¹æ³•ï¼Œå…¶ä¸­ GRPO ç­‰åŸºäºåˆ†ç»„çš„æ–¹æ³•åŠå…¶å˜ä½“è¢«å¹¿æ³›é‡‡ç”¨ã€‚è¿™ç±»æ–¹æ³•é€šè¿‡ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡æ¥é¿å…è®­ç»ƒä»·å€¼å‡½æ•°ï¼ˆcriticï¼‰ï¼Œä½†å…¶ç†è®ºæ€§è´¨å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡æ­ç¤ºäº†åŸºäºåˆ†ç»„çš„å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªæ ¹æœ¬æ€§é—®é¢˜ï¼šç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å™¨ç›¸å¯¹äºçœŸå®ï¼ˆæœŸæœ›ï¼‰ä¼˜åŠ¿å­˜åœ¨å›ºæœ‰åå·®ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥åå·®ä¼šç³»ç»Ÿæ€§åœ°å‰Šå¼±ä¸­ç­‰éš¾åº¦é—®é¢˜çš„å­¦ä¹ ä¿¡å·ï¼ŒåŒæ—¶è¿‡åº¦å¼ºåŒ–ç®€å•å’Œå›°éš¾é—®é¢˜çš„ä¿¡å·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å†å²æ„ŸçŸ¥çš„è‡ªé€‚åº”éš¾åº¦åŠ æƒæ–¹æ³•ï¼ˆHA-DWï¼‰ï¼Œé€šè¿‡åŠ¨æ€æ›´æ–°çš„éš¾åº¦é”šç‚¹æ¥è°ƒæ•´ä¼˜åŠ¿æƒé‡ã€‚åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHA-DW æŒç»­ä¼˜äº GRPO åŠå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚"
            },
            "highlights": ["RLVR", "GRPO", "biased", "HA-DW"]
          }
        }
      ]
    },
    {
      "type": "Section",
      "props": { "title": { "en": "Key Contributions", "zh": "æ ¸å¿ƒè´¡çŒ®" }, "icon": "star" },
      "children": [
        {
          "type": "ContributionList",
          "props": {
            "items": [
              {
                "badge": { "en": "Theory", "zh": "ç†è®º" },
                "title": { "en": "Bias Analysis of Group-Relative Advantage", "zh": "ç»„ç›¸å¯¹ä¼˜åŠ¿çš„åå·®åˆ†æ" },
                "description": { "en": "First rigorous theoretical analysis revealing systematic bias in group-based advantage estimation under RLVR settings", "zh": "é¦–ä¸ªä¸¥æ ¼çš„ç†è®ºåˆ†æï¼Œæ­ç¤º RLVR åœºæ™¯ä¸‹åŸºäºåˆ†ç»„çš„ä¼˜åŠ¿ä¼°è®¡ä¸­å­˜åœ¨ç³»ç»Ÿæ€§åå·®" }
              },
              {
                "badge": { "en": "Algorithm", "zh": "ç®—æ³•" },
                "title": { "en": "HA-DW (History-Aware Adaptive Difficulty Weighting)", "zh": "HA-DWï¼ˆå†å²æ„ŸçŸ¥çš„è‡ªé€‚åº”éš¾åº¦åŠ æƒï¼‰" },
                "description": { "en": "Novel method that dynamically adjusts advantage weights using an evolving difficulty anchor computed from training history", "zh": "åˆ©ç”¨è®­ç»ƒå†å²åŠ¨æ€æ›´æ–°éš¾åº¦é”šç‚¹ï¼Œè‡ªé€‚åº”è°ƒæ•´ä¼˜åŠ¿æƒé‡çš„æ–°æ–¹æ³•" }
              },
              {
                "badge": { "en": "Empirical", "zh": "å®éªŒ" },
                "title": { "en": "Comprehensive Evaluation", "zh": "å…¨é¢çš„å®éªŒéªŒè¯" },
                "description": { "en": "Validated on 5 mathematical reasoning benchmarks (GSM8K, MATH500, Minerva, GaoKao, OlympiadBench) across model scales (1.5B-7B)", "zh": "åœ¨ 5 ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ï¼ˆGSM8Kã€MATH500ã€Minervaã€é«˜è€ƒã€OlympiadBenchï¼‰ä¸ŠéªŒè¯ï¼Œè¦†ç›– 1.5B è‡³ 7B å¤šç§æ¨¡å‹è§„æ¨¡" }
              }
            ],
            "numbered": true
          }
        }
      ]
    },
    {
      "type": "Section",
      "props": { "title": { "en": "Background & Problem", "zh": "èƒŒæ™¯ä¸é—®é¢˜" }, "icon": "info" },
      "children": [
        {
          "type": "Prose",
          "props": {
            "content": {
              "en": "**Reinforcement Learning from Verifier Rewards (RLVR)** has become a dominant paradigm for improving LLM reasoning capabilities. Unlike RLHF which requires learned reward models, RLVR uses rule-based verifiers that can provide exact correctness signals for problems with verifiable answers.\n\n**Group-based methods** like GRPO avoid learning value functions by estimating advantages relative to other samples in the same batch. For each question, multiple responses are sampled and their rewards are normalized within the group.\n\nHowever, this seemingly elegant approach has a hidden flaw that we expose in this work.",
              "zh": "**åŸºäºéªŒè¯å™¨å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰**å·²æˆä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸»æµèŒƒå¼ã€‚ä¸éœ€è¦è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„ RLHF ä¸åŒï¼ŒRLVR ä½¿ç”¨åŸºäºè§„åˆ™çš„éªŒè¯å™¨ï¼Œèƒ½å¤Ÿå¯¹æœ‰ç¡®å®šç­”æ¡ˆçš„é—®é¢˜ç»™å‡ºç²¾ç¡®çš„æ­£è¯¯åˆ¤å®šä¿¡å·ã€‚\n\n**åŸºäºåˆ†ç»„çš„æ–¹æ³•**ï¼ˆå¦‚ GRPOï¼‰æ— éœ€å­¦ä¹ ä»·å€¼å‡½æ•°ï¼Œè€Œæ˜¯é€šè¿‡åŒæ‰¹æ¬¡å†…æ ·æœ¬é—´çš„ç›¸å¯¹æ¯”è¾ƒæ¥ä¼°è®¡ä¼˜åŠ¿å€¼ã€‚å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œé‡‡æ ·å¤šæ¡å›ç­”å¹¶åœ¨ç»„å†…å¯¹å¥–åŠ±è¿›è¡Œæ ‡å‡†åŒ–ã€‚\n\nç„¶è€Œï¼Œè¿™ä¸€çœ‹ä¼¼ä¼˜é›…çš„åšæ³•èƒŒåéšè—ç€ä¸€ä¸ªç¼ºé™·ï¼Œæœ¬æ–‡å°†å¯¹æ­¤è¿›è¡Œæ·±å…¥åˆ†æã€‚"
            }
          }
        },
        {
          "type": "Callout",
          "props": {
            "type": "warning",
            "title": { "en": "The Hidden Problem", "zh": "éšè—çš„é—®é¢˜" },
            "content": { "en": "Group-relative advantage estimation introduces systematic bias: questions of moderate difficulty are disadvantaged, while both easy and hard questions receive over-weighted advantages.", "zh": "ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å¼•å…¥äº†ç³»ç»Ÿæ€§åå·®ï¼šä¸­ç­‰éš¾åº¦çš„é—®é¢˜è¢«ä½ä¼°ï¼Œè€Œè¿‡äºç®€å•æˆ–è¿‡äºå›°éš¾çš„é—®é¢˜åè€Œè·å¾—äº†è¿‡é«˜çš„ä¼˜åŠ¿æƒé‡ã€‚" }
          }
        }
      ]
    },
    {
      "type": "Section",
      "props": { "title": { "en": "Theoretical Analysis", "zh": "ç†è®ºåˆ†æ" }, "icon": "bulb" },
      "children": [
        {
          "type": "DefinitionList",
          "props": {
            "items": [
              {
                "term": { "en": "True Advantage A(q)", "zh": "çœŸå®ä¼˜åŠ¿ A(q)" },
                "definition": { "en": "The expected advantage of a correct response to question q under the true distribution", "zh": "åœ¨çœŸå®åˆ†å¸ƒä¸‹ï¼Œé—®é¢˜ q çš„æ­£ç¡®å›ç­”æ‰€å¯¹åº”çš„æœŸæœ›ä¼˜åŠ¿å€¼" }
              },
              {
                "term": { "en": "Group-Relative Advantage Ã‚(q)", "zh": "ç»„ç›¸å¯¹ä¼˜åŠ¿ Ã‚(q)" },
                "definition": { "en": "The empirical advantage computed by normalizing rewards within a batch of sampled responses", "zh": "é€šè¿‡åœ¨é‡‡æ ·å›ç­”çš„æ‰¹æ¬¡å†…è¿›è¡Œå¥–åŠ±æ ‡å‡†åŒ–æ‰€å¾—åˆ°çš„ç»éªŒä¼˜åŠ¿å€¼" }
              },
              {
                "term": { "en": "Difficulty C(q)", "zh": "éš¾åº¦ C(q)" },
                "definition": { "en": "The probability of generating a correct response for question q under the current policy Ï€", "zh": "å½“å‰ç­–ç•¥ Ï€ ä¸‹ï¼Œé’ˆå¯¹é—®é¢˜ q ç”Ÿæˆæ­£ç¡®å›ç­”çš„æ¦‚ç‡" }
              }
            ]
          }
        },
        {
          "type": "Theorem",
          "props": {
            "type": "theorem",
            "number": "1",
            "title": { "en": "Bias of Group-Relative Advantage", "zh": "ç»„ç›¸å¯¹ä¼˜åŠ¿çš„åå·®" },
            "content": { "en": "Let C(q) denote the difficulty of question q under policy Ï€. The group-relative advantage estimator Ã‚(q) has expected bias E[Ã‚(q)] - A(q) = f(C(q)) where f is a non-linear function that is negative for moderate difficulties (C â‰ˆ 0.5) and positive for extreme difficulties (C â†’ 0 or C â†’ 1).", "zh": "è®¾ C(q) ä¸ºç­–ç•¥ Ï€ ä¸‹é—®é¢˜ q çš„éš¾åº¦ã€‚ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å™¨ Ã‚(q) çš„æœŸæœ›åå·® E[Ã‚(q)] - A(q) = f(C(q))ï¼Œå…¶ä¸­ f ä¸ºéçº¿æ€§å‡½æ•°ï¼šåœ¨ä¸­ç­‰éš¾åº¦ï¼ˆC â‰ˆ 0.5ï¼‰æ—¶å–è´Ÿå€¼ï¼Œåœ¨æç«¯éš¾åº¦ï¼ˆC â†’ 0 æˆ– C â†’ 1ï¼‰æ—¶å–æ­£å€¼ã€‚" }
          }
        },
        {
          "type": "Formula",
          "props": {
            "latex": "\\mathbb{E}[\\hat{A}(q)] - A(q) = \\frac{1-2C(q)}{G \\cdot C(q)(1-C(q))} + O(G^{-2})",
            "block": true,
            "label": "1"
          }
        },
        {
          "type": "Prose",
          "props": {
            "content": {
              "en": "This theorem reveals that:\n\n- **Easy questions (C â†’ 1)**: Positive bias amplifies advantages\n- **Hard questions (C â†’ 0)**: Positive bias also amplifies advantages  \n- **Moderate questions (C â‰ˆ 0.5)**: Negative bias suppresses learning\n\nThe U-shaped bias curve means moderate-difficulty questions, which are often the most valuable for learning, receive systematically weaker gradient signals.",
              "zh": "è¯¥å®šç†è¡¨æ˜ï¼š\n\n- **ç®€å•é—®é¢˜ï¼ˆC â†’ 1ï¼‰**ï¼šæ­£åå·®æ”¾å¤§äº†ä¼˜åŠ¿ä¿¡å·\n- **å›°éš¾é—®é¢˜ï¼ˆC â†’ 0ï¼‰**ï¼šæ­£åå·®åŒæ ·æ”¾å¤§äº†ä¼˜åŠ¿ä¿¡å·\n- **ä¸­ç­‰éš¾åº¦é—®é¢˜ï¼ˆC â‰ˆ 0.5ï¼‰**ï¼šè´Ÿåå·®å‹åˆ¶äº†å­¦ä¹ ä¿¡å·\n\nè¿™æ¡ U å‹åå·®æ›²çº¿æ„å‘³ç€ï¼Œæœ€å…·å­¦ä¹ ä»·å€¼çš„ä¸­ç­‰éš¾åº¦é—®é¢˜åè€Œè·å¾—äº†æ›´å¼±çš„æ¢¯åº¦ä¿¡å·ã€‚"
            }
          }
        },
        {
          "type": "Figure",
          "props": {
            "images": [
              {
                "src": "https://arxiv.org/html/2601.08521v1/x1.png",
                "alt": { "en": "Bias curve showing U-shaped relationship between difficulty and bias", "zh": "U å‹åå·®æ›²çº¿ï¼šéš¾åº¦ä¸åå·®çš„å…³ç³»" },
                "width": "600px"
              }
            ],
            "label": { "en": "Figure 1", "zh": "å›¾ 1" },
            "caption": { "en": "The bias of group-relative advantage estimation as a function of question difficulty C(q). Moderate-difficulty questions (C â‰ˆ 0.5) suffer negative bias while extreme difficulties receive positive bias.", "zh": "ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡çš„åå·®éšé—®é¢˜éš¾åº¦ C(q) å˜åŒ–çš„æ›²çº¿ã€‚ä¸­ç­‰éš¾åº¦é—®é¢˜ï¼ˆC â‰ˆ 0.5ï¼‰å—åˆ°è´Ÿåå·®å½±å“ï¼Œæç«¯éš¾åº¦é—®é¢˜åˆ™è·å¾—æ­£åå·®ã€‚" }
          }
        }
      ]
    },
    {
      "type": "Section",
      "props": { "title": { "en": "Proposed Method: HA-DW", "zh": "æ‰€ææ–¹æ³•ï¼šHA-DW" }, "icon": "code" },
      "children": [
        {
          "type": "MethodOverview",
          "props": {
            "steps": [
              { "step": 1, "title": { "en": "Maintain Difficulty Anchor", "zh": "ç»´æŠ¤éš¾åº¦é”šç‚¹" }, "description": { "en": "Track running estimate C_t of average difficulty across training batches", "zh": "é€šè¿‡æ»‘åŠ¨ä¼°è®¡è¿½è¸ªå„è®­ç»ƒæ‰¹æ¬¡çš„å¹³å‡éš¾åº¦ C_t" } },
              { "step": 2, "title": { "en": "Compute Reweighting Factor", "zh": "è®¡ç®—åŠ æƒç³»æ•°" }, "description": { "en": "For each question, compute w(q) based on deviation from C_t", "zh": "æ ¹æ®æ¯ä¸ªé—®é¢˜ä¸é”šç‚¹ C_t çš„åç¦»ç¨‹åº¦è®¡ç®—æƒé‡ w(q)" } },
              { "step": 3, "title": { "en": "Apply to Advantages", "zh": "åŠ æƒè°ƒæ•´ä¼˜åŠ¿å€¼" }, "description": { "en": "Multiply group-relative advantages by w(q) before policy update", "zh": "åœ¨ç­–ç•¥æ›´æ–°å‰ï¼Œå°†ç»„ç›¸å¯¹ä¼˜åŠ¿å€¼ä¹˜ä»¥æƒé‡ w(q)" } },
              { "step": 4, "title": { "en": "Update Anchor", "zh": "æ›´æ–°é”šç‚¹" }, "description": { "en": "Update C_t using exponential moving average of batch difficulties", "zh": "ä½¿ç”¨æŒ‡æ•°æ»‘åŠ¨å¹³å‡æ›´æ–°é”šç‚¹ C_t" } }
            ]
          }
        },
        {
          "type": "Algorithm",
          "props": {
            "title": { "en": "HA-DW Training Loop", "zh": "HA-DW è®­ç»ƒæµç¨‹" },
            "steps": [
              { "line": 1, "code": "Initialize: policy Ï€, difficulty anchor Câ‚€ = 0.5, momentum Î± = 0.9" },
              { "line": 2, "code": "for t = 1 to T do:" },
              { "line": 3, "code": "Sample batch B = {qâ‚, ..., qâ‚™}", "indent": 1 },
              { "line": 4, "code": "for each qáµ¢ âˆˆ B do:", "indent": 1 },
              { "line": 5, "code": "Sample G responses, compute Äˆ(qáµ¢) = #correct/G", "indent": 2 },
              { "line": 6, "code": "Compute group-relative advantage Ã‚(qáµ¢)", "indent": 2 },
              { "line": 7, "code": "Compute weight wáµ¢ = exp(-|Äˆ(qáµ¢) - Câ‚œ|/Ï„)", "indent": 2 },
              { "line": 8, "code": "Update policy with weighted advantage wáµ¢ Â· Ã‚(qáµ¢)", "indent": 1 },
              { "line": 9, "code": "Update anchor: Câ‚œâ‚Šâ‚ = Î±Â·Câ‚œ + (1-Î±)Â·mean(Äˆ(qáµ¢))", "indent": 1 }
            ],
            "caption": { "en": "The evolving anchor Câ‚œ adapts to the policy's improving capabilities during training", "zh": "é”šç‚¹ Câ‚œ éšè®­ç»ƒè¿‡ç¨‹åŠ¨æ€è°ƒæ•´ï¼Œä»¥é€‚åº”ç­–ç•¥èƒ½åŠ›çš„æŒç»­æå‡" }
          }
        },
        {
          "type": "Formula",
          "props": {
            "latex": "w(q) = \\exp\\left(-\\frac{|\\hat{C}(q) - C_t|}{\\tau}\\right)",
            "block": true,
            "label": "2"
          }
        },
        {
          "type": "Callout",
          "props": {
            "type": "tip",
            "title": { "en": "Why History-Aware?", "zh": "ä¸ºä»€ä¹ˆéœ€è¦å†å²æ„ŸçŸ¥ï¼Ÿ" },
            "content": { "en": "The difficulty anchor Câ‚œ evolves during training as the policy improves. This prevents the reweighting from becoming stale and ensures moderate-difficulty questions (relative to current policy capability) are always appropriately weighted.", "zh": "éš¾åº¦é”šç‚¹ Câ‚œ éšç­–ç•¥èƒ½åŠ›çš„æå‡è€ŒåŠ¨æ€è°ƒæ•´ã€‚è¿™é¿å…äº†æƒé‡åˆ†é…è¿‡æ—¶çš„é—®é¢˜ï¼Œç¡®ä¿ç›¸å¯¹äºå½“å‰ç­–ç•¥è€Œè¨€çš„ä¸­ç­‰éš¾åº¦é—®é¢˜å§‹ç»ˆè·å¾—åˆé€‚çš„æƒé‡ã€‚" }
          }
        }
      ]
    },
    {
      "type": "Section",
      "props": { "title": { "en": "Experimental Results", "zh": "å®éªŒç»“æœ" }, "icon": "chart" },
      "children": [
        {
          "type": "MetricsGrid",
          "props": {
            "metrics": [
              { "label": { "en": "Benchmarks", "zh": "è¯„æµ‹åŸºå‡†" }, "value": 5, "icon": "check" },
              { "label": { "en": "Model Scales", "zh": "æ¨¡å‹è§„æ¨¡" }, "value": "1.5B-7B" },
              { "label": { "en": "Avg. Improvement", "zh": "å¹³å‡æå‡" }, "value": "+2.3%", "trend": "up" },
              { "label": { "en": "Best Gain", "zh": "æœ€å¤§å¢å¹…" }, "value": "+4.1%", "trend": "up", "suffix": " on MATH500" }
            ],
            "cols": 4
          }
        },
        {
          "type": "ResultsTable",
          "props": {
            "caption": { "en": "Table 1: Performance comparison on mathematical reasoning benchmarks (accuracy %)", "zh": "è¡¨ 1ï¼šæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„æ€§èƒ½å¯¹æ¯”ï¼ˆå‡†ç¡®ç‡ %ï¼‰" },
            "columns": [
              { "key": "method", "label": { "en": "Method", "zh": "æ–¹æ³•" } },
              { "key": "gsm8k", "label": "GSM8K" },
              { "key": "math500", "label": "MATH500", "highlight": true },
              { "key": "minerva", "label": "Minerva" },
              { "key": "gaokao", "label": { "en": "GaoKao", "zh": "é«˜è€ƒ" } },
              { "key": "olympiad", "label": "OlympiadBench" }
            ],
            "rows": [
              { "method": "SFT Baseline", "gsm8k": "72.3", "math500": "34.2", "minerva": "28.1", "gaokao": "45.6", "olympiad": "12.4" },
              { "method": "GRPO", "gsm8k": "78.5", "math500": "41.6", "minerva": "35.2", "gaokao": "51.3", "olympiad": "18.7" },
              { "method": "Dr. GRPO", "gsm8k": "79.1", "math500": "42.3", "minerva": "36.1", "gaokao": "52.0", "olympiad": "19.2" },
              { "method": "DAPO", "gsm8k": "79.8", "math500": "43.1", "minerva": "36.8", "gaokao": "52.7", "olympiad": "19.8" },
              { "method": "HA-DW (Ours)", "gsm8k": "81.2", "math500": "45.7", "minerva": "38.4", "gaokao": "54.1", "olympiad": "21.3" }
            ],
            "highlights": [
              { "row": 4, "col": "gsm8k" },
              { "row": 4, "col": "math500" },
              { "row": 4, "col": "minerva" },
              { "row": 4, "col": "gaokao" },
              { "row": 4, "col": "olympiad" }
            ]
          }
        },
        {
          "type": "Figure",
          "props": {
            "images": [
              {
                "src": "https://arxiv.org/html/2601.08521v1/x2.png",
                "alt": { "en": "Training curves comparing GRPO vs HA-DW", "zh": "GRPO ä¸ HA-DW è®­ç»ƒæ›²çº¿å¯¹æ¯”" },
                "width": "100%"
              }
            ],
            "label": { "en": "Figure 2", "zh": "å›¾ 2" },
            "caption": { "en": "Training curves on MATH500. HA-DW shows faster convergence and higher final accuracy compared to GRPO and other baselines.", "zh": "MATH500 è®­ç»ƒæ›²çº¿ã€‚HA-DW ç›¸æ¯” GRPO åŠå…¶ä»–åŸºçº¿æ–¹æ³•æ”¶æ•›æ›´å¿«ï¼Œæœ€ç»ˆå‡†ç¡®ç‡æ›´é«˜ã€‚" }
          }
        }
      ]
    },
    {
      "type": "Section",
      "props": { "title": { "en": "Ablation Studies", "zh": "æ¶ˆèå®éªŒ" }, "icon": "chart" },
      "children": [
        {
          "type": "ResultsTable",
          "props": {
            "caption": { "en": "Table 2: Ablation study on HA-DW components (MATH500 accuracy %)", "zh": "è¡¨ 2ï¼šHA-DW å„ç»„ä»¶æ¶ˆèå®éªŒï¼ˆMATH500 å‡†ç¡®ç‡ %ï¼‰" },
            "columns": [
              { "key": "variant", "label": { "en": "Variant", "zh": "å˜ä½“" } },
              { "key": "accuracy", "label": { "en": "Accuracy", "zh": "å‡†ç¡®ç‡" } },
              { "key": "delta", "label": { "en": "Î” vs Full", "zh": "Î” ç›¸æ¯”å®Œæ•´ç‰ˆ" } }
            ],
            "rows": [
              { "variant": "Full HA-DW", "accuracy": "45.7", "delta": "â€”" },
              { "variant": "w/o History (fixed C=0.5)", "accuracy": "43.8", "delta": "-1.9" },
              { "variant": "w/o Adaptive Weight", "accuracy": "42.9", "delta": "-2.8" },
              { "variant": "w/o Both (= GRPO)", "accuracy": "41.6", "delta": "-4.1" }
            ],
            "highlights": [
              { "row": 0, "col": "accuracy" }
            ]
          }
        },
        {
          "type": "Callout",
          "props": {
            "type": "important",
            "title": { "en": "Key Finding", "zh": "å…³é”®å‘ç°" },
            "content": { "en": "Both components (history-aware anchor and adaptive weighting) contribute significantly. The history-aware anchor alone provides +1.2% while adaptive weighting alone provides +0.9%. Their combination yields the full +4.1% improvement.", "zh": "ä¸¤ä¸ªç»„ä»¶ï¼ˆå†å²æ„ŸçŸ¥é”šç‚¹å’Œè‡ªé€‚åº”åŠ æƒï¼‰å‡æœ‰æ˜¾è‘—è´¡çŒ®ï¼šä»…ä½¿ç”¨å†å²æ„ŸçŸ¥é”šç‚¹å¯æå‡ +1.2%ï¼Œä»…ä½¿ç”¨è‡ªé€‚åº”åŠ æƒå¯æå‡ +0.9%ï¼ŒäºŒè€…ç»“åˆå®ç°äº†å®Œæ•´çš„ +4.1% æå‡ã€‚" }
          }
        }
      ]
    },
    {
      "type": "Section",
      "props": { "title": { "en": "Key Takeaways", "zh": "è¦ç‚¹æ€»ç»“" }, "icon": "bulb" },
      "children": [
        {
          "type": "Grid",
          "props": { "cols": 2 },
          "children": [
            {
              "type": "KeyPoint",
              "props": {
                "icon": "warning",
                "title": { "en": "Bias in Group-Relative Methods", "zh": "åˆ†ç»„æ–¹æ³•çš„å›ºæœ‰åå·®" },
                "description": { "en": "GRPO and similar methods have inherent bias that disadvantages moderate-difficulty questions", "zh": "GRPO ç­‰æ–¹æ³•å­˜åœ¨å›ºæœ‰åå·®ï¼Œå¯¼è‡´ä¸­ç­‰éš¾åº¦é—®é¢˜çš„å­¦ä¹ ä¿¡å·è¢«å‰Šå¼±" },
                "variant": "warning"
              }
            },
            {
              "type": "KeyPoint",
              "props": {
                "icon": "check",
                "title": { "en": "Simple Fix, Big Impact", "zh": "æ”¹åŠ¨å°ï¼Œæ•ˆæœå¤§" },
                "description": { "en": "HA-DW adds minimal overhead (<5% training time) while consistently improving performance", "zh": "HA-DW ä»…å¢åŠ ä¸åˆ° 5% çš„è®­ç»ƒå¼€é”€ï¼Œå³å¯å¸¦æ¥ç¨³å®šçš„æ€§èƒ½æå‡" },
                "variant": "success"
              }
            }
          ]
        },
        {
          "type": "Highlight",
          "props": {
            "text": { "en": "The group-relative advantage estimator is inherently biased relative to the true (expected) advantage. HA-DW corrects this bias through history-aware adaptive difficulty weighting.", "zh": "ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å™¨ç›¸å¯¹äºçœŸå®ï¼ˆæœŸæœ›ï¼‰ä¼˜åŠ¿å­˜åœ¨å›ºæœ‰åå·®ã€‚HA-DW é€šè¿‡å†å²æ„ŸçŸ¥çš„è‡ªé€‚åº”éš¾åº¦åŠ æƒæœºåˆ¶çº æ­£äº†è¿™ä¸€åå·®ã€‚" },
            "type": "important",
            "source": { "en": "Main contribution", "zh": "æ ¸å¿ƒè´¡çŒ®" }
          }
        }
      ]
    },
    {
      "type": "Section",
      "props": { "title": { "en": "Links & Resources", "zh": "é“¾æ¥ä¸èµ„æº" }, "icon": "link" },
      "children": [
        {
          "type": "LinkGroup",
          "props": {
            "links": [
              { "href": "https://arxiv.org/abs/2601.08521", "label": { "en": "arXiv Paper", "zh": "arXiv è®ºæ–‡" }, "icon": "arxiv" },
              { "href": "https://arxiv.org/pdf/2601.08521", "label": "PDF", "icon": "pdf" },
              { "href": "https://arxiv.org/html/2601.08521v1", "label": { "en": "HTML Version", "zh": "HTML ç‰ˆæœ¬" }, "icon": "link" },
              { "href": "https://github.com/example/ha-dw", "label": { "en": "Code (Coming)", "zh": "ä»£ç ï¼ˆå³å°†å‘å¸ƒï¼‰" }, "icon": "github" }
            ]
          }
        }
      ]
    },
    {
      "type": "BrandFooter",
      "props": {
        "timestamp": "2026-01-27T15:30:00Z",
        "attribution": "Powered by ActionBook",
        "disclaimer": { "en": "This report was automatically generated by AI based on the original paper content. Please refer to the original paper for complete details and citations.", "zh": "æœ¬æŠ¥å‘Šç”± AI æ ¹æ®åŸå§‹è®ºæ–‡å†…å®¹è‡ªåŠ¨ç”Ÿæˆï¼Œå¦‚éœ€å®Œæ•´ç»†èŠ‚å’Œå¼•ç”¨è¯·å‚é˜…åŸå§‹è®ºæ–‡ã€‚" }
      }
    }
  ]
}
